{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiQA Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim to analyze the sentiment of news headlines related to the financial field, utilizing cutting-edge NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import os module\n",
    "\n",
    "def load_fiqa_data(data_dir):\n",
    "    \"\"\"Loads headline and post data from the FiQA dataset.\"\"\"\n",
    "    headline_file = os.path.join(data_dir, \"task1_headline_ABSA_train.json\")\n",
    "    post_file = os.path.join(data_dir, \"task1_post_ABSA_train.json\")\n",
    "\n",
    "    headline_df = pd.read_json(headline_file)\n",
    "    post_df = pd.read_json(post_file)\n",
    "    return headline_df, post_df\n",
    "\n",
    "# Example usage (assuming your data is in a 'data' subdirectory):\n",
    "headline_df, post_df = load_fiqa_data(\"data/FiQA_ABSA_task1\")\n",
    "\n",
    "headline_df.head(5)\n",
    "post_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import rcParams\n",
    "import scipy.sparse # Import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "# plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiqaDataLoading:\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "    \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"Load JSON data and convert to structured DataFrame\"\"\"\n",
    "        with open(self.filepath, encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        records = []\n",
    "        for entry_id, content in raw_data.items():\n",
    "            for info in content['info']:\n",
    "                record = {\n",
    "                    'id': entry_id,\n",
    "                    'sentence': content['sentence'],\n",
    "                    'snippets': info['snippets'],\n",
    "                    'target': info['target'],\n",
    "                    'sentiment_score': float(info['sentiment_score']),\n",
    "                    'aspects': self.safe_literal_eval(info['aspects'])\n",
    "                }\n",
    "                records.append(record)\n",
    "\n",
    "        self.df = pd.DataFrame(records)\n",
    "        self._enhance_data()\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def safe_literal_eval(self, x):\n",
    "        try:\n",
    "            if isinstance(x, str):\n",
    "                return ast.literal_eval(x)\n",
    "            else:\n",
    "                return []\n",
    "        except (SyntaxError, ValueError):\n",
    "            if isinstance(x, str):\n",
    "                return [x]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "    def _enhance_data(self):\n",
    "        \"\"\"Create additional features and clean data\"\"\"\n",
    "\n",
    "        # Sentiment classification\n",
    "        bins = [-1, -0.33, 0.33, 1]\n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        self.df['sentiment_class'] = pd.cut(self.df['sentiment_score'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "        # Aspect hierarchy processing\n",
    "        def extract_aspect(aspects, level):\n",
    "            try:\n",
    "                if aspects and isinstance(aspects[0], str):\n",
    "                    return aspects[0].split('/')[level] if len(aspects[0].split('/')) > level else None\n",
    "                else:\n",
    "                    return None\n",
    "            except (IndexError, TypeError):\n",
    "                return None\n",
    "\n",
    "        self.df['primary_aspect'] = self.df['aspects'].apply(lambda x: extract_aspect(x, 0))\n",
    "        self.df['secondary_aspect'] = self.df['aspects'].apply(lambda x: extract_aspect(x, 1))\n",
    "\n",
    "        self.df['snippet_text'] = self.df['snippets'].apply(lambda x: ' '.join(self.safe_literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiqaEDA:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def _validate_data(self):\n",
    "        \"\"\"Data quality checks\"\"\"\n",
    "        print(\"=== Data Validation Report ===\")\n",
    "        print(f\"Total entries: {len(self.df)}\")\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(self.df.isnull().sum())\n",
    "\n",
    "        # Check for invalid sentiment scores (outside -1 to 1 range)\n",
    "        invalid_scores = self.df[(self.df['sentiment_score'] < -1) | (self.df['sentiment_score'] > 1)]\n",
    "        print(f\"\\nInvalid sentiment scores (outside -1 to 1): {len(invalid_scores)}\")\n",
    "\n",
    "        # Check for empty snippets or sentences\n",
    "        print(f\"\\nEmpty snippets: {len(self.df[self.df['snippet_text'] == ''])}\")\n",
    "        print(f\"Empty sentences: {len(self.df[self.df['sentence'] == ''])}\")\n",
    "\n",
    "\n",
    "    def analyze_sentiment_distribution(self):\n",
    "        \"\"\"Generate sentiment visualizations\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(self.df['sentiment_score'], bins=20, kde=True)\n",
    "        plt.title('Sentiment Score Distribution')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.df['sentiment_class'].value_counts().plot(kind='bar')\n",
    "        plt.title('Sentiment Class Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def analyze_aspects(self):\n",
    "        \"\"\"Aspect category analysis\"\"\"\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Primary aspects\n",
    "        primary_counts = self.df['primary_aspect'].value_counts()\n",
    "        sns.barplot(y=primary_counts.index, x=primary_counts.values, ax=ax[0])\n",
    "        ax[0].set_title('Primary Aspect Distribution')\n",
    "        \n",
    "        # Secondary aspects (top 15)\n",
    "        secondary_counts = self.df['secondary_aspect'].value_counts().head(15)\n",
    "        sns.barplot(y=secondary_counts.index, x=secondary_counts.values, ax=ax[1])\n",
    "        ax[1].set_title('Top 15 Secondary Aspects')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def generate_word_clouds(self):\n",
    "        \"\"\"Generate sentiment-specific word clouds with stopword removal\"\"\"\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        stopwords = set([\"a\", \"as\", \"and\", \"by\", \"on\", \"of\", \"to\", \"the\", \"in\", \"for\", \"with\", \"Ã‚\"])\n",
    "\n",
    "        for i, sentiment in enumerate(['positive', 'neutral', 'negative']):\n",
    "            text = ' '.join(self.df[self.df['sentiment_class'] == sentiment]['snippet_text'])\n",
    "            wc = WordCloud(width=1200, height=800, \n",
    "                          background_color='white', stopwords=stopwords,\n",
    "                          colormap='viridis' if sentiment == 'neutral' else\n",
    "                          'Greens' if sentiment == 'positive' else 'Reds',\n",
    "                          max_words=200).generate(text)\n",
    "\n",
    "            ax[i].imshow(wc)\n",
    "            ax[i].set_title(f'{sentiment.capitalize()} Sentiment Terms')\n",
    "            ax[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def analyze_targets(self):\n",
    "        \"\"\"Company/organization analysis\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        target_counts = self.df['target'].value_counts().head(10)\n",
    "        sns.barplot(y=target_counts.index, x=target_counts.values)\n",
    "        plt.title('Top 10 Frequently Mentioned Targets')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"Execute complete EDA pipeline\"\"\"\n",
    "        self._validate_data()\n",
    "        \n",
    "        print(\"\\n=== Basic Statistics ===\")\n",
    "        print(self.df.describe(include='all'))\n",
    "        \n",
    "        self.analyze_sentiment_distribution()\n",
    "        self.analyze_aspects()\n",
    "        self.generate_word_clouds()\n",
    "        self.analyze_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FiqaDataLoading(\"data/FiQA_ABSA_task1/task1_headline_ABSA_train.json\")\n",
    "df = loader.load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "DEFAULT_STOPWORDS = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValueHandler:\n",
    "    \"\"\"Handles all missing value operations\"\"\"\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        print(\"\\n=== Handling Missing Values ===\")\n",
    "        original_rows = len(df)\n",
    "        \n",
    "        df = self._remove_critical_nulls(df)\n",
    "        df = self._handle_aspect_nulls(df)\n",
    "        # Removed snippet null handler as snippets are generated later\n",
    "        \n",
    "        print(f\"Removed {original_rows - len(df)} rows with missing critical data\")\n",
    "        print(f\"Final dataset shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def _remove_critical_nulls(self, df):\n",
    "        return df.dropna(subset=['sentence', 'sentiment_score', 'target'])\n",
    "    \n",
    "    def _handle_aspect_nulls(self, df):\n",
    "        df['aspects'] = df['aspects'].apply(lambda x: ['Unknown'] if len(x) == 0 else x)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextColumnProcessor:\n",
    "    def __init__(self, text_cleaner):\n",
    "        self.text_cleaner = text_cleaner\n",
    "    \n",
    "    def process_text_columns(self, df):\n",
    "        print(\"\\n=== Cleaning Text Data ===\")\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Clean the main sentence while preserving structure\n",
    "        df['clean_sentence'] = df['sentence'].apply(self.text_cleaner.clean_text)\n",
    "        \n",
    "        # Extract meaningful snippets\n",
    "        df['snippets'] = df['clean_sentence'].apply(self._extract_snippets)\n",
    "        \n",
    "        print(\"Text cleaning complete\")\n",
    "        return df\n",
    "    \n",
    "    def _extract_snippets(self, text):\n",
    "        \"\"\"Extract meaningful snippets based on text patterns\"\"\"\n",
    "        # Split on common business news patterns\n",
    "        splits = re.split(r'(?:\\.|;|:|!|\\\\?|\\\\n| - )', text) # Improved split regex\\n,\n",
    "        \n",
    "        # Clean up splits\n",
    "        snippets = []\n",
    "        for snippet in splits:\n",
    "            snippet = snippet.strip()\n",
    "            if snippet and len(snippet.split()) >= 2:  # Minimum 2 words\n",
    "                snippets.append(snippet)\n",
    "        \n",
    "        # Ensure we have at least one snippet\n",
    "        if not snippets and text:\n",
    "            snippets = [text]\n",
    "        \n",
    "        return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Handles all text cleaning operations\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model, lemmatize=True, remove_stopwords=True):\n",
    "        self.nlp_model = nlp_model\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = self._remove_html(text)\n",
    "        text = self._remove_special_chars(text)\n",
    "        text = self._normalize_whitespace(text)\n",
    "        return self._process_with_spacy(text.lower())\n",
    "\n",
    "    def _remove_html(self, text):\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    def _remove_special_chars(self, text):\n",
    "        # Keeps periods, commas, and other meaningful punctuation\n",
    "        return re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "\n",
    "    def _normalize_whitespace(self, text):\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    def _process_with_spacy(self, text):\n",
    "        doc = self.nlp_model(text)\n",
    "        tokens = [\n",
    "            token.lemma_.strip() if self.lemmatize else token.text\n",
    "            for token in doc\n",
    "            if not (self.remove_stopwords and token.is_stop) and token.text.strip() != '' # Added condition to filter empty strings\n",
    "        ]\n",
    "        return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Handles data validation operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def validate_sentiment_scores(self, df):\n",
    "        self._check_score_range(df)\n",
    "        return self._cross_validate_with_vader(df)\n",
    "\n",
    "    def _check_score_range(self, df):\n",
    "        invalid_scores = df[~df['sentiment_score'].between(-1, 1)]\n",
    "        if not invalid_scores.empty:\n",
    "            raise ValueError(f\"Found {len(invalid_scores)} invalid sentiment scores\")\n",
    "\n",
    "    def _cross_validate_with_vader(self, df):\n",
    "        df['score_discrepancy'] = df.apply(self._calculate_discrepancy, axis=1)\n",
    "        self._report_discrepancies(df)\n",
    "        return df.drop(columns=['score_discrepancy'])\n",
    "\n",
    "    def _calculate_discrepancy(self, row):\n",
    "        text = \" \".join(row['snippets'])\n",
    "        vader_score = self.sia.polarity_scores(text)['compound']\n",
    "        return abs(row['sentiment_score'] - vader_score)\n",
    "    \n",
    "    def _report_discrepancies(self, df):\n",
    "        # Consider adjusting this threshold if 0.5 is too strict\n",
    "        high_discrepancy = df[df['score_discrepancy'] > 0.5]\n",
    "        if not high_discrepancy.empty:\n",
    "            print(f\"Warning: {len(high_discrepancy)} entries with significant score discrepancy\")\n",
    "            print(high_discrepancy[['sentence', 'sentiment_score', 'score_discrepancy']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBalancer:\n",
    "    \"\"\"Handles class balancing operations\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def balance_classes(self, df):\n",
    "        print(\"\\n=== Balancing Classes ===\")\n",
    "        self._print_distribution(df, \"Original\")\n",
    "        \n",
    "        balanced_df = self._perform_multiclass_balancing(df)\n",
    "        self._print_distribution(balanced_df, \"Balanced\")\n",
    "        \n",
    "        return balanced_df\n",
    "\n",
    "    def _perform_multiclass_balancing(self, df):\n",
    "        class_counts = df['sentiment_class'].value_counts()\n",
    "        target_size = int(class_counts.median())  # Use median as target\n",
    "        \n",
    "        balanced_dfs = []\n",
    "        for class_name in class_counts.index:\n",
    "            class_df = df[df['sentiment_class'] == class_name]\n",
    "            if len(class_df) > target_size:\n",
    "                class_df = class_df.sample(n=target_size, random_state=self.random_state)\n",
    "            elif len(class_df) < target_size:\n",
    "                class_df = resample(\n",
    "                    class_df,\n",
    "                    replace=True,\n",
    "                    n_samples=target_size,\n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "            balanced_dfs.append(class_df)\n",
    "        \n",
    "        return pd.concat(balanced_dfs)\n",
    "\n",
    "    def _print_distribution(self, df, stage):\n",
    "        print(f\"\\n{stage} Class Distribution:\")\n",
    "        print(df['sentiment_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Orchestrates the entire preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 text_clean=True,\n",
    "                 lemmatize=True,\n",
    "                 remove_stopwords=True,\n",
    "                 handle_imbalance=False,\n",
    "                 random_state=42):\n",
    "        self.text_clean = text_clean\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.handle_imbalance = handle_imbalance\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize components\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.text_cleaner = TextCleaner(self.nlp, lemmatize, remove_stopwords)\n",
    "        self.text_processor = TextColumnProcessor(self.text_cleaner)\n",
    "        self.missing_handler = MissingValueHandler()\n",
    "        self.validator = DataValidator()\n",
    "        self.balancer = DataBalancer(random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df = self.missing_handler.handle_missing_values(df)\n",
    "        \n",
    "        if self.text_clean:\n",
    "            df = self.text_processor.process_text_columns(df)\n",
    "            \n",
    "        df = self.validator.validate_sentiment_scores(df)\n",
    "        \n",
    "        if self.handle_imbalance:\n",
    "            df = self.balancer.balance_classes(df)\n",
    "            \n",
    "        return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor(\n",
    "        text_clean=True,\n",
    "        lemmatize=True,\n",
    "        remove_stopwords=True,\n",
    "        handle_imbalance=True\n",
    "    )\n",
    "    \n",
    "cleaned_data = preprocessor.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_DIM = 300  # Set your desired embedding dimension\n",
    "EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
    "MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from spacy.lang.en import English\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.decomposition import PCA # Import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeatureExtractor(ABC, TransformerMixin):\n",
    "    \"\"\"Abstract base class for feature extractors\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, X):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer(BaseFeatureExtractor):\n",
    "    \"\"\"Handles traditional text vectorization methods\"\"\"\n",
    "    \n",
    "    def __init__(self, method='tfidf', max_features=5000, ngram_range=(1,2)):\n",
    "        self.method = method\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        texts = self._preprocess_text(X)\n",
    "        \n",
    "        if self.method == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(max_features=self.max_features, \n",
    "                                            ngram_range=self.ngram_range)\n",
    "        else:\n",
    "            self.vectorizer = CountVectorizer(max_features=self.max_features, \n",
    "                                            ngram_range=self.ngram_range)\n",
    "        self.vectorizer.fit(texts)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        texts = self._preprocess_text(X) \n",
    "        return self.vectorizer.transform(texts)\n",
    "    \n",
    "    def _preprocess_text(self, X):\n",
    "        def clean_text(snippets):\n",
    "            # Join snippets\n",
    "            text = ' '.join(snippets)\n",
    "            \n",
    "            # Basic cleaning\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove numbers but keep important financial numbers\n",
    "            # Improved regex to handle broader range of financial numbers\n",
    "            text = re.sub(r'\\b(?<!\\$)\\d+(?!\\s*(?:million|billion|trillion|percent|%))\\b', '', text)\n",
    "            \n",
    "            # Normalize financial terms\n",
    "            text = re.sub(r'(\\$|Â£|â‚¬)', 'currency_symbol', text)\n",
    "            text = re.sub(r'\\b(million|mn)\\b', 'million', text)\n",
    "            text = re.sub(r'\\b(billion|bn)\\b', 'billion', text)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "            \n",
    "        return X['snippets'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(BaseFeatureExtractor):\n",
    "    \"\"\"Handles word embeddings (GloVe/Word2Vec/FastText)\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_path, embedding_dim=300):\n",
    "        self.embedding_path = embedding_path\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_model = None\n",
    "        self.nlp = English()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if not os.path.exists(self.embedding_path):\n",
    "            # Download the GloVe embeddings if not present\n",
    "            import gensim.downloader as api\n",
    "            \n",
    "            print(f\"Downloading GloVe embeddings from {self.embedding_path}\")\n",
    "            self.embedding_model = api.load(self.embedding_path)\n",
    "            # Save the downloaded embeddings for future use\n",
    "            self.embedding_model.save_word2vec_format(self.embedding_path + \".word2vec\") # Correct file extension\n",
    "        elif 'glove' in self.embedding_path.lower():\n",
    "            # Convert GloVe format to word2vec format if not already done\n",
    "            word2vec_path = self.embedding_path + '.word2vec' # Correct file extension\n",
    "            if not os.path.exists(word2vec_path):\n",
    "                print(f\"Converting GloVe to Word2Vec format: {self.embedding_path}\")\n",
    "                glove2word2vec(self.embedding_path, word2vec_path)\n",
    "            self.embedding_model = KeyedVectors.load_word2vec_format(word2vec_path)\n",
    "        else:\n",
    "            self.embedding_model = KeyedVectors.load_word2vec_format(self.embedding_path)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        def document_to_vector(texts):\n",
    "            vectors = []\n",
    "            for text in texts:\n",
    "                tokens = [token.text for token in self.nlp(text)]\n",
    "                word_vectors = [self.embedding_model[word] for word in tokens \n",
    "                               if word in self.embedding_model]\n",
    "                if len(word_vectors) > 0:\n",
    "                    vectors.append(np.mean(word_vectors, axis=0))\n",
    "                else:\n",
    "                    vectors.append(np.zeros(self.embedding_dim))\n",
    "            return np.array(vectors)\n",
    "        \n",
    "        texts = X['snippets'].apply(lambda x: ' '.join(x))\n",
    "        return document_to_vector(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbeddings(BaseFeatureExtractor):\n",
    "    \"\"\"Generates contextual embeddings using transformer models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', batch_size=32, max_length=128, device='cpu'): # Added device\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device # Added device\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device) # Added to(self.device)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        texts = X['snippets'].apply(lambda x: ' '.join(x)).tolist()\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i+self.batch_size]\n",
    "            inputs = self.tokenizer(batch, return_tensors='pt', \n",
    "                                  padding=True, truncation=True,\n",
    "                                  max_length=self.max_length).to(self.device) # Added to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        return np.concatenate(embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinguisticFeatureExtractor(BaseFeatureExtractor):\n",
    "    \"\"\"Extracts syntactic and lexical features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = English()\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.pos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        texts = X['snippets'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            feature_dict = {\n",
    "                'num_chars': len(text),\n",
    "                'num_words': len(list(doc)),\n",
    "                'num_sentences': len(list(doc.sents)),\n",
    "                **self._get_pos_counts(doc),\n",
    "                **self._get_sentiment_scores(text)\n",
    "            }\n",
    "            features.append(feature_dict)\n",
    "            \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def _get_pos_counts(self, doc):\n",
    "        counts = {f\"pos_{tag}\": 0 for tag in self.pos_tags}\n",
    "        for token in doc:\n",
    "            if token.pos_ in counts:\n",
    "                counts[f\"pos_{token.pos_}\"] += 1\n",
    "        return counts\n",
    "    \n",
    "    def _get_sentiment_scores(self, text):\n",
    "        scores = self.sia.polarity_scores(text)\n",
    "        return {\n",
    "            'sentiment_neg': scores['neg'],\n",
    "            'sentiment_neu': scores['neu'],\n",
    "            'sentiment_pos': scores['pos'],\n",
    "            'sentiment_compound': scores['compound']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessFeatureExtractor(BaseFeatureExtractor):\n",
    "    \"\"\"Extracts business-specific features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        # Add financial patterns matcher\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self._add_patterns()\n",
    "        \n",
    "        # Common financial metrics\n",
    "        self.financial_metrics = ['revenue', 'profit', 'earnings', 'ebitda', 'sales',\n",
    "                                'margin', 'growth', 'market share', 'dividend']\n",
    "        \n",
    "        # Business actions\n",
    "        self.business_actions = ['merger', 'acquisition', 'partnership', 'investment',\n",
    "                               'divestment', 'restructuring', 'layoff', 'expansion']\n",
    "        \n",
    "        # Market indicators\n",
    "        self.market_indicators = ['bull', 'bear', 'volatile', 'surge', 'plunge',\n",
    "                                'rally', 'correction', 'uptick', 'downturn']\n",
    "    \n",
    "    def _add_patterns(self):\n",
    "        # Add patterns for common financial expressions\n",
    "        money_patterns = [\n",
    "            [{'LIKE_NUM': True}, {'TEXT': {'IN': ['million', 'billion', 'trillion']}}],\n",
    "            [{'SYMBOL': {'IN': ['$', 'Â£', 'â‚¬']}}, {'LIKE_NUM': True}]\n",
    "        ]\n",
    "        self.matcher.add('MONEY_AMOUNT', money_patterns)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        texts = X['snippets'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        for text in texts:\n",
    "            doc = self.nlp(text)\n",
    "            feature_dict = {\n",
    "                **self._get_financial_metrics(doc),\n",
    "                **self._get_business_actions(doc),\n",
    "                **self._get_market_indicators(doc),\n",
    "                **self._get_monetary_mentions(doc),\n",
    "                **self._get_temporal_features(doc),\n",
    "                **self._get_company_features(doc)\n",
    "            }\n",
    "            features.append(feature_dict)\n",
    "            \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def _get_financial_metrics(self, doc):\n",
    "        return {f\"metric_{metric.replace(' ', '_')}\": 1 if metric in doc.text.lower() else 0\n",
    "                for metric in self.financial_metrics}\n",
    "    \n",
    "    def _get_business_actions(self, doc):\n",
    "        return {f\"action_{action}\": 1 if action in doc.text.lower() else 0\n",
    "                for action in self.business_actions}\n",
    "    \n",
    "    def _get_market_indicators(self, doc):\n",
    "        return {f\"indicator_{indicator}\": 1 if indicator in doc.text.lower() else 0\n",
    "                for indicator in self.market_indicators}\n",
    "    \n",
    "    def _get_monetary_mentions(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        return {\n",
    "            'has_monetary_value': len(matches) > 0,\n",
    "            'monetary_mention_count': len(matches)\n",
    "        }\n",
    "    \n",
    "    def _get_temporal_features(self, doc):\n",
    "        # Extract temporal expressions\n",
    "        time_indicators = ['year', 'quarter', 'month', 'week', 'day']\n",
    "        return {\n",
    "            'has_temporal_reference': any(indicator in doc.text.lower() for indicator in time_indicators),\n",
    "            'future_reference': any(word in doc.text.lower() for word in ['will', 'plan', 'expect', 'forecast'])\n",
    "        }\n",
    "    \n",
    "    def _get_company_features(self, doc):\n",
    "        return {\n",
    "            'company_count': len([ent for ent in doc.ents if ent.label_ == 'ORG']),\n",
    "            'has_multiple_companies': len([ent for ent in doc.ents if ent.label_ == 'ORG']) > 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCombiner(BaseFeatureExtractor):\n",
    "    \"\"\"Combines different feature sets and handles feature selection\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_extractors, n_components=0.95):\n",
    "        self.feature_extractors = feature_extractors\n",
    "        self.n_components = n_components\n",
    "        self.pca = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Fit all extractors\n",
    "        features = []\n",
    "        for extractor in self.feature_extractors:\n",
    "            if hasattr(extractor, 'fit'):\n",
    "                extractor.fit(X, y)\n",
    "            feat = extractor.transform(X)\n",
    "            features.append(feat)\n",
    "            \n",
    "        # Combine features\n",
    "        combined = self._combine_features(features)\n",
    "        \n",
    "        # Apply PCA if needed\n",
    "        if self.n_components:\n",
    "            self.pca = PCA(n_components=self.n_components)\n",
    "            self.pca.fit(combined)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        for extractor in self.feature_extractors:\n",
    "            feat = extractor.transform(X)\n",
    "            features.append(feat)\n",
    "            \n",
    "        combined = self._combine_features(features)\n",
    "        \n",
    "        if self.pca:\n",
    "            combined = self.pca.transform(combined)\n",
    "            \n",
    "        return combined\n",
    "    \n",
    "    def _combine_features(self, features):\n",
    "        # Convert sparse matrices to dense if needed\n",
    "        dense_features = []\n",
    "        for feat in features:\n",
    "            if isinstance(feat, scipy.sparse.spmatrix):\n",
    "                dense_features.append(feat.toarray())\n",
    "            elif isinstance(feat, pd.DataFrame):\n",
    "                dense_features.append(feat.values)\n",
    "            elif isinstance(feat, np.ndarray): # Added this condition\n",
    "                dense_features.append(feat)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported feature type: {type(feat)}\") # Added type error for unsupported types\n",
    "                \n",
    "        return np.hstack(dense_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"Orchestrates multiple feature extraction methods\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_config):\n",
    "        self.feature_extractors = []\n",
    "        self._validate_config(feature_config)\n",
    "        \n",
    "    def _validate_config(self, config):\n",
    "        for extractor_type, params in config.items():\n",
    "            if extractor_type == 'bow':\n",
    "                self.feature_extractors.append(\n",
    "                    TextVectorizer(method='bow', **params)\n",
    "                )\n",
    "            elif extractor_type == 'tfidf': \n",
    "                self.feature_extractors.append(\n",
    "                    TextVectorizer(method='tfidf', **params)\n",
    "                )\n",
    "            elif extractor_type == 'embeddings':\n",
    "                self.feature_extractors.append(\n",
    "                    EmbeddingVectorizer(**params)\n",
    "                )\n",
    "            elif extractor_type == 'transformers':\n",
    "                self.feature_extractors.append(\n",
    "                    TransformerEmbeddings(**params)\n",
    "                )\n",
    "            elif extractor_type == 'linguistic':\n",
    "                self.feature_extractors.append(\n",
    "                    LinguisticFeatureExtractor(**params)\n",
    "                )\n",
    "            elif extractor_type == 'business':\n",
    "                self.feature_extractors.append(\n",
    "                    BusinessFeatureExtractor(**params)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid extractor type: {extractor_type}\")\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        features = []\n",
    "        for extractor in self.feature_extractors:\n",
    "            transformed = extractor.fit_transform(X)\n",
    "            if isinstance(transformed, np.ndarray):\n",
    "                df = pd.DataFrame(transformed)\n",
    "            elif isinstance(transformed, pd.DataFrame):\n",
    "                df = transformed\n",
    "            elif isinstance(transformed, scipy.sparse.spmatrix): # Corrected condition\n",
    "                df = pd.DataFrame(transformed.toarray())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported feature type: {type(transformed)}\") # Added type error for unsupported types\n",
    "            features.append(df.add_prefix(f\"{type(extractor).__name__}_\"))\n",
    "            \n",
    "        return pd.concat(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_config = {\n",
    "    'tfidf': {\n",
    "        'max_features': 1000,\n",
    "        'ngram_range': (1, 2)\n",
    "    },\n",
    "    'embeddings': {\n",
    "        'embedding_path': EMBEDDING_FILE,\n",
    "        'embedding_dim': EMBEDDING_DIM\n",
    "    },\n",
    "    'transformers': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
    "    },\n",
    "    'linguistic': {},\n",
    "    'business': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = FeatureEngineeringPipeline(feature_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = feature_pipeline.fit_transform(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, features, targets, test_size=0.2, random_state=42): # Added random_state\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state # Added random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            features, targets,\n",
    "            test_size=test_size,\n",
    "            stratify=targets,\n",
    "            random_state=self.random_state # Use random_state here\n",
    "        )\n",
    "        \n",
    "    def add_model(self, name, model):\n",
    "        self.models[name] = model\n",
    "        \n",
    "    def _evaluate(self, y_true, y_pred):\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        return {\n",
    "            'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "            'accuracy': np.mean(y_true == y_pred)\n",
    "        }\n",
    "    \n",
    "    def run_cross_validation(self, model, cv=5):\n",
    "        skf = StratifiedKFold(cv, shuffle=True, random_state=self.random_state) # Added shuffle and random_state\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(self.X_train, self.y_train):\n",
    "            X_fold_train = self.X_train.iloc[train_idx] # Use iloc for DataFrames\n",
    "            y_fold_train = self.y_train.iloc[train_idx] # Use iloc for DataFrames\n",
    "            X_val = self.X_train.iloc[val_idx] # Use iloc for DataFrames\n",
    "            y_val = self.y_train.iloc[val_idx] # Use iloc for DataFrames\n",
    "            \n",
    "            model.train(X_fold_train, y_fold_train, X_val, y_val)\n",
    "            preds = model.predict(X_val)\n",
    "            scores.append(self._evaluate(y_val, preds))\n",
    "            \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def train_all(self, use_cross_validation=True):\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\n=== Training {name} ===\")\n",
    "            \n",
    "            if use_cross_validation:\n",
    "                cv_score = self.run_cross_validation(model)\n",
    "                print(f\"CV Score: {cv_score}\")\n",
    "                \n",
    "            # Final training on full data\n",
    "            final_score = model.train(self.X_train, self.y_train, self.X_test, self.y_test) # Use test data for validation\n",
    "            test_preds = model.predict(self.X_test)\n",
    "            test_metrics = self._evaluate(self.y_test, test_preds)\n",
    "            \n",
    "            self.results[name] = {\n",
    "                'cv_score': cv_score if use_cross_validation else None,\n",
    "                'test_metrics': test_metrics,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "    def save_best_model(self, metric='f1_weighted', file_prefix = \"best_model\"):\n",
    "        \"\"\"Saves the best model based on a specified metric.\"\"\"\n",
    "        if not self.results:\n",
    "            raise ValueError(\"No models trained yet. Call 'train_all' first.\")\n",
    "\n",
    "        best_name = max(self.results, key=lambda k: self.results[k]['test_metrics'][metric])\n",
    "        best_model = self.results[best_name]['model']\n",
    "        filepath = f\"{file_prefix}_{best_name}.pkl\"\n",
    "        joblib.dump(best_model, filepath) # Use joblib for saving models\n",
    "        print(f\"Saved best model ({metric}): {best_name} to {filepath}\")\n",
    "        \n",
    "        # Save associated data for reproducibility\n",
    "        best_model_data = self.results[best_name]\n",
    "        if best_model_data.get('cv_score'):\n",
    "            print(f\"Best model's CV score: {best_model_data.get('cv_score')}\")\n",
    "        print(f\"Best model's test metrics: {best_model_data['test_metrics']}\")\n",
    "        print(f\"Best model's parameters: {best_model.best_params_ if hasattr(best_model, 'best_params_') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalMLModel(BaseModel):\n",
    "    \"\"\"Handles traditional machine learning models\"\"\"\n",
    "    \n",
    "    MODEL_TYPES = {\n",
    "        'svm': SVC,\n",
    "        'random_forest': RandomForestClassifier,\n",
    "        'gradient_boosting': GradientBoostingClassifier\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_type='random_forest', param_space=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_type = model_type\n",
    "        self.param_space = param_space or self._default_param_space()\n",
    "        self.scoring = 'f1_weighted'\n",
    "        \n",
    "    def _default_param_space(self):\n",
    "        if self.model_type == 'svm':\n",
    "            return {\n",
    "                'C': (1e-3, 1e3, 'log-uniform'),\n",
    "                'gamma': (1e-4, 1e-1, 'log-uniform')\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'n_estimators': (100, 1000),\n",
    "                'max_depth': (3, 10),\n",
    "                'learning_rate': (0.01, 0.3) if self.model_type == 'gradient_boosting' else (None, None) # Conditional hyperparameter\n",
    "            }\n",
    "    \n",
    "    def build_model(self, **kwargs):\n",
    "        model_class = self.MODEL_TYPES[self.model_type]\n",
    "        return model_class(\n",
    "            random_state=self.random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        opt = BayesSearchCV(\n",
    "            estimator=self.build_model(),\n",
    "            search_spaces=self.param_space,\n",
    "            n_iter=30,\n",
    "            cv=3,\n",
    "            scoring=self.scoring,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1 # Use all available cores for faster training\n",
    "        )\n",
    "        \n",
    "        opt.fit(X_train, y_train)\n",
    "        self.model = opt.best_estimator_\n",
    "        self.best_params_ = opt.best_params_\n",
    "        return opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningModel(BaseModel):\n",
    "    \"\"\"Handles deep learning architectures\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='lstm', input_shape=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_type = model_type\n",
    "        self.input_shape = input_shape\n",
    "        self.history = None\n",
    "        \n",
    "    def build_model(self, **params):\n",
    "        if self.model_type == 'lstm':\n",
    "            return self._build_lstm(**params)\n",
    "        elif self.model_type == 'cnn':\n",
    "            return self._build_cnn(**params)\n",
    "        elif self.model_type == 'transformer':\n",
    "            return self._build_transformer()\n",
    "            \n",
    "    def _build_lstm(self, units=64, dropout=0.2):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = LSTM(units, return_sequences=True)(inputs)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        outputs = Dense(3, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def _build_cnn(self, filters=64, kernel_size=3):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = Conv1D(filters, kernel_size, activation='relu')(inputs)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def _build_transformer(self):\n",
    "        try:\n",
    "            model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=3\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=Adam(3e-5),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            return model\n",
    "        except ValueError as e: # Catch potential import errors\n",
    "            print(\"Error loading transformer model:\\n\", e)\n",
    "            return None\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        if self.model_type == 'transformer': # Special handling for transformer training\n",
    "            if self.model is None: # Check if the model was loaded correctly\n",
    "                return 0.0 # Return a default score if the model couldn't be loaded\n",
    "            \n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=3, restore_best_weights=True),\n",
    "                ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "            ]\n",
    "\n",
    "            self.history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=20,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            return max(self.history.history['val_accuracy'])\n",
    "        else:\n",
    "            self.model = self.build_model()\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=3, restore_best_weights=True),\n",
    "                ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "            ]\n",
    "\n",
    "            self.history = self.model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=20,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            return max(self.history.history['val_accuracy'])\n",
    "\n",
    "    def predict(self, X): # Modified predict function to handle different model types\n",
    "        if self.model_type == 'transformer':\n",
    "            # Transformer models require tokenization\n",
    "            inputs = self.tokenizer(X, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            return predictions\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def save(self, path): # Modified saving method for transformer models\n",
    "        if self.model_type == 'transformer':\n",
    "            self.model.save_pretrained(path)\n",
    "            self.tokenizer.save_pretrained(path)\n",
    "        else:\n",
    "            joblib.dump(self.model, path)\n",
    "\n",
    "    def load(self, path): # Modified loading method for transformer models\n",
    "        if self.model_type == 'transformer':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "            self.model = TFAutoModelForSequenceClassification.from_pretrained(path)\n",
    "        else:\n",
    "            self.model = joblib.load(path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
